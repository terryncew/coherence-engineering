# runtime/bi_modal_controller.py
import asyncio
import time
import numpy as np
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, Tuple, List
from enum import Enum
import threading
from collections import deque

class ProcessingMode(Enum):
    EXPLORATION = "exploration"
    CONSERVATION = "conservation"
    REFLEX = "reflex"
    DAMAGE_CONTROL = "damage_control"

@dataclass
class BoundarySignals:
    """Real-time system health metrics"""
    phi_star: float = 0.0        # coherence per cost
    vkd: float = 1.0             # viability / safety margin (>0 = safe)
    kappa_hat: float = 0.0       # curvature index (0-1, higher = more stress)
    epsilon: float = 0.0         # entropy leak proxy
    # Telemetry
    token_rate: float = 0.0
    queue_depth: int = 0
    contradiction_rate: float = 0.0
    retrieval_mismatch: float = 0.0
    phi_volatility: float = 0.0
    timestamp: float = field(default_factory=time.time)

@dataclass
class LungOutput:
    """Output from either fast or slow processing lung"""
    content: str
    confidence: float
    tokens_used: int
    processing_time: float
    mode: str
    metadata: Dict[str, Any] = field(default_factory=dict)

class BiModalController:
    """
    Dual-lung runtime controller that optimizes coherence-per-cost (phi_star)
    while respecting VKD (safety) boundaries.
    """
    def __init__(self,
                 alpha_kappa: float = 0.5,
                 alpha_epsilon: float = 0.3,
                 delta: float = 0.1,
                 eta1: float = 0.1,   # phi* gradient learning rate
                 eta2: float = 0.2,   # kappa response rate
                 eta3: float = 0.15,  # epsilon response rate
                 update_interval: float = 1.0):
        self.alpha_kappa = alpha_kappa
        self.alpha_epsilon = alpha_epsilon
        self.delta = delta
        self.eta1 = eta1
        self.eta2 = eta2
        self.eta3 = eta3
        self.update_interval = update_interval

        # State
        self.beta = 0.5  # mixing: 0 = all fast, 1 = all slow
        self.signals_history: deque[BoundarySignals] = deque(maxlen=100)
        self.phi_history: deque[float] = deque(maxlen=20)

        # Control loop
        self._running = False
        self._control_thread: Optional[threading.Thread] = None

    def start_control_loop(self):
        """Start the continuous control loop"""
        self._running = True
        self._control_thread = threading.Thread(target=self._control_loop, daemon=True)
        self._control_thread.start()

    def stop_control_loop(self):
        """Stop the control loop"""
        self._running = False
        if self._control_thread:
            self._control_thread.join()

    def _control_loop(self):
        """Main control loop - updates beta based on system signals"""
        while self._running:
            try:
                if len(self.signals_history) >= 2:
                    self._update_beta()
                time.sleep(self.update_interval)
            except Exception as e:
                print(f"Control loop error: {e}")

    def _update_beta(self):
        """Update mixing coefficient beta based on current signals"""
        current = self.signals_history[-1]

        # Hard safety: VKD breach -> fast-only
        if current.vkd < 0:
            self.beta = 0.0
            return

        # Approximate dphi/dbeta by recent slope
        if len(self.phi_history) >= 2:
            dphi_dt = self.phi_history[-1] - self.phi_history[-2]
            dphi_dbeta = dphi_dt
        else:
            dphi_dbeta = 0.0

        # Curvature rate of change
        if len(self.signals_history) >= 2:
            dkappa_dt = (self.signals_history[-1].kappa_hat - self.signals_history[-2].kappa_hat) / max(self.update_interval, 1e-6)
        else:
            dkappa_dt = 0.0

        # Control law
        beta_delta = (self.eta1 * np.sign(dphi_dbeta)
                      - self.eta2 * dkappa_dt
                      - self.eta3 * current.epsilon)

        self.beta = float(np.clip(self.beta + beta_delta, 0.0, 1.0))

    def calculate_phi_star(self, signals: BoundarySignals) -> float:
        """Calculate phi* using the Bridge-style equation"""
        I_c = max(0.0, 1.0 - signals.contradiction_rate)  # simple coherence proxy
        denom = (1.0 + self.alpha_kappa * signals.kappa_hat
                      + self.alpha_epsilon * signals.epsilon
                      + self.delta)
        return I_c / denom

    def update_signals(self, signals: BoundarySignals):
        """Update system signals and recalc phi*"""
        signals.phi_star = self.calculate_phi_star(signals)
        self.signals_history.append(signals)
        self.phi_history.append(signals.phi_star)

    async def route_request(self, request: str, context: Optional[Dict[str, Any]] = None) -> Tuple[LungOutput, LungOutput, float]:
        """Route a request through both lungs and return outputs + current beta"""
        context = context or {}

        fast_task = asyncio.create_task(self._fast_lung_process(request, context))
        slow_task = asyncio.create_task(self._slow_lung_process(request, context))

        try:
            fast_output = await asyncio.wait_for(fast_task, timeout=5.0)

            slow_timeout = 30.0 * self.beta if self.beta > 0.1 else 1.0
            try:
                slow_output = await asyncio.wait_for(slow_task, timeout=slow_timeout)
            except asyncio.TimeoutError:
                slow_output = LungOutput(
                    content="[TIMEOUT]",
                    confidence=0.0,
                    tokens_used=0,
                    processing_time=slow_timeout,
                    mode="slow_timeout"
                )
        except asyncio.TimeoutError:
            fast_output = LungOutput(
                content="[SYSTEM_OVERLOAD]",
                confidence=0.0,
                tokens_used=0,
                processing_time=5.0,
                mode="fast_timeout"
            )
            slow_output = fast_output

        return fast_output, slow_output, self.beta

    async def _fast_lung_process(self, request: str, context: Dict[str, Any]) -> LungOutput:
        start = time.time()
        max_tokens = int(200 * (1 - self.beta))
        temperature = 0.25
        await asyncio.sleep(0.1 + np.random.exponential(0.2))
        safe_content = self._generate_safe_response(request, max_tokens)
        return LungOutput(
            content=safe_content,
            confidence=0.7,
            tokens_used=max_tokens // 2,
            processing_time=time.time() - start,
            mode="fast_reflex",
            metadata={"temperature": temperature, "max_tokens": max_tokens}
        )

    async def _slow_lung_process(self, request: str, context: Dict[str, Any]) -> LungOutput:
        start = time.time()
        if self.beta < 0.1 or (self.signals_history and self.signals_history[-1].vkd < 0.5):
            return LungOutput(
                content="[INHIBITED]",
                confidence=0.0,
                tokens_used=0,
                processing_time=0.001,
                mode="slow_inhibited"
            )
        max_tokens = int(800 * self.beta)
        temperature = 0.6 + 0.4 * self.beta
        processing_delay = 1.0 + np.random.exponential(2.0 * self.beta)
        await asyncio.sleep(processing_delay)
        rich_content = self._generate_rich_response(request, max_tokens, temperature)
        return LungOutput(
            content=rich_content,
            confidence=0.85,
            tokens_used=max_tokens // 3,
            processing_time=time.time() - start,
            mode="slow_deliberative",
            metadata={"temperature": temperature, "max_tokens": max_tokens}
        )

    def _generate_safe_response(self, request: str, max_tokens: int) -> str:
        templates = [
            f"Based on standard guidelines, here's a concise response to '{request[:50]}...'",
            f"Following established patterns, I can provide: [structured answer for '{request[:30]}...']",
            f"Using cached knowledge, the key points are: [safe summary for '{request[:40]}...']"
        ]
        return str(np.random.choice(templates))

    def _generate_rich_response(self, request: str, max_tokens: int, temperature: float) -> str:
        approaches = [
            f"Let me explore this deeply. For '{request[:30]}...', I'll consider multiple angles: [detailed analysis with tools, retrieval, reasoning ...]",
            f"This requires nuanced thinking about '{request[:40]}...'. Drawing from broader context: [comprehensive response with verification, examples, counterpoints ...]",
            f"Taking time to reason through '{request[:35]}...': [step-by-step analysis with tool use, checks, synthesis ...]"
        ]
        return str(np.random.choice(approaches))
